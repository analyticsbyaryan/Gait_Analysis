{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "df= pd.read_csv(r\"D:\\DEPAUL\\OTHER\\PROJECTS\\Gait analysis\\Dataset\\Gait_analysis_cleaned.csv\")\n",
    "#df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['Person_ID'])  \n",
    "Y = df['Person_ID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified k-Fold CV Accuracy - Model 1 (Baseline): 0.9792 ± 0.0295\n",
      "Stratified k-Fold CV Accuracy - Model 2 (Optimized): 0.9792 ± 0.0295\n",
      "Stratified k-Fold CV Accuracy - Model 3 (Regularized): 1.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report  # ✅ Import classification_report\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model2 = RandomForestClassifier(max_depth=10, min_samples_leaf= 1, min_samples_split=2, n_estimators=300)\n",
    "model3 = RandomForestClassifier(\n",
    "    max_depth=5,                # Restrict depth to prevent memorization\n",
    "    min_samples_split=5,        # Require at least 5 samples to split a node\n",
    "    min_samples_leaf=3,         # Require at least 3 samples per leaf\n",
    "    n_estimators=200,           # Reduce the number of trees to 200 (avoid excessive complexity)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define Stratified K-Fold (ensures balanced distribution)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Store accuracy scores\n",
    "scores_m1 = []\n",
    "scores_m2 = []\n",
    "scores_m3 = []\n",
    "\n",
    "# Perform Stratified k-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    # Initialize Models\n",
    "    model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model2 = RandomForestClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300)\n",
    "    model3 = RandomForestClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=3, n_estimators=200, random_state=42)\n",
    "\n",
    "    # Train models\n",
    "    model1.fit(X_train, Y_train)\n",
    "    model2.fit(X_train, Y_train)\n",
    "    model3.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict\n",
    "    Y_pred_m1 = model1.predict(X_test)\n",
    "    Y_pred_m2 = model2.predict(X_test)\n",
    "    Y_pred_m3 = model3.predict(X_test)\n",
    "\n",
    "    # Store accuracy scores\n",
    "    scores_m1.append(accuracy_score(Y_test, Y_pred_m1))\n",
    "    scores_m2.append(accuracy_score(Y_test, Y_pred_m2))\n",
    "    scores_m3.append(accuracy_score(Y_test, Y_pred_m3))\n",
    "\n",
    "# Print final results\n",
    "print(f\"Stratified k-Fold CV Accuracy - Model 1 (Baseline): {np.mean(scores_m1):.4f} ± {np.std(scores_m1):.4f}\")\n",
    "print(f\"Stratified k-Fold CV Accuracy - Model 2 (Optimized): {np.mean(scores_m2):.4f} ± {np.std(scores_m2):.4f}\")\n",
    "print(f\"Stratified k-Fold CV Accuracy - Model 3 (Regularized): {np.mean(scores_m3):.4f} ± {np.std(scores_m3):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Test Accuracy - Model 1 (Baseline): 1.0000\n",
      "Holdout Test Accuracy - Model 2 (Optimized): 1.0000\n",
      "Holdout Test Accuracy - Model 3 (Regularized): 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure the dataset is shuffled properly before splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=df[\"Person_ID\"], random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model2 = RandomForestClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300)\n",
    "model3 = RandomForestClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=3, n_estimators=200, random_state=42)\n",
    "\n",
    "# Train models\n",
    "model1.fit(X_train, Y_train)\n",
    "model2.fit(X_train, Y_train)\n",
    "model3.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "Y_pred_m1 = model1.predict(X_test)\n",
    "Y_pred_m2 = model2.predict(X_test)\n",
    "Y_pred_m3 = model3.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Holdout Test Accuracy - Model 1 (Baseline): {accuracy_score(Y_test, Y_pred_m1):.4f}\")\n",
    "print(f\"Holdout Test Accuracy - Model 2 (Optimized): {accuracy_score(Y_test, Y_pred_m2):.4f}\")\n",
    "print(f\"Holdout Test Accuracy - Model 3 (Regularized): {accuracy_score(Y_test, Y_pred_m3):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARING FEATURE IMPORTANCE ACROSS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features Across Models:\n",
      "\n",
      "  Model 1 (Baseline) Model 2 (Optimized) Model 3 (Regularized)\n",
      "0                114                 116                   226\n",
      "1                141                 216                   251\n",
      "2                178                 251                   220\n",
      "3                176                 222                   114\n",
      "4                100                 115                   250\n",
      "5                 84                 239                    36\n",
      "6                226                 267                   178\n",
      "7                250                 304                   163\n",
      "8                220                 220                   100\n",
      "9                 35                  82                   318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature importance from all models\n",
    "feature_importances_m1 = model1.feature_importances_\n",
    "feature_importances_m2 = model2.feature_importances_\n",
    "feature_importances_m3 = model3.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices_m1 = np.argsort(feature_importances_m1)[::-1]\n",
    "sorted_indices_m2 = np.argsort(feature_importances_m2)[::-1]\n",
    "sorted_indices_m3 = np.argsort(feature_importances_m3)[::-1]\n",
    "\n",
    "# Get top 10 features for each model\n",
    "top_features_m1 = feature_names[sorted_indices_m1[:10]]\n",
    "top_features_m2 = feature_names[sorted_indices_m2[:10]]\n",
    "top_features_m3 = feature_names[sorted_indices_m3[:10]]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "feature_df = pd.DataFrame({\n",
    "    \"Model 1 (Baseline)\": top_features_m1,\n",
    "    \"Model 2 (Optimized)\": top_features_m2,\n",
    "    \"Model 3 (Regularized)\": top_features_m3\n",
    "})\n",
    "\n",
    "# Print the feature comparison\n",
    "print(\"Top 10 Features Across Models:\\n\")\n",
    "print(feature_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features in All Models:\n",
      "  Common Features\n",
      "0             220\n"
     ]
    }
   ],
   "source": [
    "# Find common features among the top 10 features of all three models\n",
    "common_features = set(top_features_m1) & set(top_features_m2) & set(top_features_m3)\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "common_features_df = pd.DataFrame({\"Common Features\": list(common_features)})\n",
    "\n",
    "# Display the common features\n",
    "print(\"Common Features in All Models:\")\n",
    "print(common_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features for Model 3:\n",
      "226: 0.0157\n",
      "251: 0.0137\n",
      "220: 0.0115\n",
      "114: 0.0112\n",
      "250: 0.0097\n",
      "36: 0.0095\n",
      "178: 0.0095\n",
      "163: 0.0090\n",
      "100: 0.0086\n",
      "318: 0.0084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature importance from Model 3\n",
    "feature_importances_m3 = model3.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importances_m3)[::-1]\n",
    "\n",
    "# Get the top 10 most important features for Model 3\n",
    "top_features_m3 = feature_names[sorted_indices[:10]]\n",
    "top_importances_m3 = feature_importances_m3[sorted_indices[:10]]\n",
    "\n",
    "# Print the top 10 features\n",
    "print(\"Top 10 Most Important Features for Model 3:\")\n",
    "for feature, importance in zip(top_features_m3, top_importances_m3):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Features Used Per Person by Model 3:\n",
      "     Feature_1 Feature_2 Feature_3 Feature_4 Feature_5\n",
      "0.0        249       250       251       252       257\n",
      "1.0        297       170       162        97        19\n",
      "2.0         61       181        45       199       129\n",
      "3.0        226       114       228       222       219\n",
      "4.0        226       132       249       145         5\n",
      "5.0        113       145       146       136       310\n",
      "6.0        107        55       281        82       161\n",
      "7.0        285       177       287       293       309\n",
      "8.0        114       120       113       125       232\n",
      "9.0        178       282       186        22       176\n",
      "10.0       272       129       300       168       286\n",
      "11.0       155       161        24       162        99\n",
      "12.0       205       141       317       226       126\n",
      "13.0       282       276       307       280       119\n",
      "14.0       287       177       265       178       275\n",
      "15.0       315       293       155       249        58\n"
     ]
    }
   ],
   "source": [
    "feature_importance_per_person = {}\n",
    "\n",
    "# Loop through each Person_ID\n",
    "for person in Y.unique():\n",
    "    # Convert Person_ID into a binary classification task (1 for this person, 0 for others)\n",
    "    Y_binary = (Y == person).astype(int)\n",
    "\n",
    "    # Train Model 3 only for this person\n",
    "    model_person = RandomForestClassifier(\n",
    "        max_depth=5, min_samples_split=5, min_samples_leaf=3, n_estimators=200, random_state=42\n",
    "    )\n",
    "    model_person.fit(X, Y_binary)\n",
    "\n",
    "    # Get feature importance\n",
    "    importance = model_person.feature_importances_\n",
    "    \n",
    "    # Store top 5 features for this person\n",
    "    top_features_person = X.columns[np.argsort(importance)[::-1][:5]]\n",
    "    feature_importance_per_person[person] = top_features_person\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "feature_person_df = pd.DataFrame.from_dict(feature_importance_per_person, orient=\"index\", columns=[f\"Feature_{i+1}\" for i in range(5)])\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop Features Used Per Person by Model 3:\")\n",
    "print(feature_person_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Commonly Used Features Across Individuals:\n",
      "249    3\n",
      "226    3\n",
      "129    2\n",
      "162    2\n",
      "161    2\n",
      "113    2\n",
      "145    2\n",
      "114    2\n",
      "178    2\n",
      "293    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Flatten all selected features\n",
    "all_selected_features = feature_person_df.values.flatten()\n",
    "\n",
    "# Count occurrences of each feature\n",
    "feature_counts = pd.Series(all_selected_features).value_counts()\n",
    "\n",
    "# Print the most commonly used features\n",
    "print(\"\\nMost Commonly Used Features Across Individuals:\")\n",
    "print(feature_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
